# A2: Bias in data

## About the project
The goal of this project is to identity if any sources of bias may exist in the Wikipedia Talk corpus datasets, and to develop testable hypotheses about how these biases might impact the behavior of machine learning models trained on the data, when those models are used for research purposes or to power data-driven applications. 

## Data Source
The data used in this project is the Wikipedia Talk corpus, and it consists of three datasets. Each dataset contains thousands of online discussion posts made by Wikipedia editors who were discussing how to write and edit Wikipedia articles. Crowdworkers labelled these posts for three kinds of hostile speech: “toxicity”, “aggression”, and “personal attacks”. 
Dataset description and schemas can be found [here](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release)

In this project, we are only using two of them: Toxicity and Personal attacks datasets, which can be found in the data folder.

## Result

## License
[Wikimedia](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release#Personal_Attacks) and [Figshare](https://figshare.com/articles/Wikipedia_Talk_Labels_Personal_Attacks/4054689)